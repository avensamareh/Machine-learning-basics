{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from scipy import optimize\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "import utils\n",
    "import scipy.io # used to load matlab-formatted .m files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = loadmat(os.path.join('ex4data1.mat'))\n",
    "\n",
    "X = data_dict['X']\n",
    "y = data_dict['y'].ravel()\n",
    "\n",
    "M = X.shape[0] # = 400 pixels per sample\n",
    "N = X.shape[1] # = 5000 samples\n",
    "L = 26 # = number of nodes in the hidden layer (including bias node)\n",
    "K = len(np.unique(y)) # = 10 distinct classes for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This gives us a 5000 by 400 matrix X \n",
    "# where every row is a training example for a handwritten digit image.\n",
    "# Randomly select 100 data points to display\n",
    "rand_indices = np.random.choice(M, 100, replace=False)\n",
    "sel = X[rand_indices, :]\n",
    "\n",
    "utils.displayData(sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones to the data matrix that allows us to treat the intercept parameter as a feature.\n",
    "X = np.hstack((np.ones((M, 1)), X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. load already trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It has 3 layers - an input layer, a hidden layer and an output layer. \n",
    "# Since the images are of size $20 * 20$, this gives us 400 input layer units\n",
    "# (not counting the extra bias unit which always outputs +1). \n",
    "# The training data was loaded into the variables X and y above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have been provided with a set of network parameters already trained.\n",
    "# These are stored in ex4weights.mat and will be loaded in the next cell of this notebook into Theta1 and Theta2. \n",
    "# The parameters have dimensions that are sized for a neural network with 25 units in the second layer\n",
    "# and 10 output units (corresponding to the 10 digit classes).\n",
    "# images are of size $20 \\times 20$, this gives us 400 input layer units\n",
    "# 5000-dimensional vector y that contains labels for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400  # 20x20 Input Images of Digits - \n",
    "hidden_layer_size = 25   # 25 hidden units\n",
    "num_labels = 10          # 10 labels, from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 401)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the weights into variables Theta1 and Theta2\n",
    "weights = scipy.io.loadmat('ex4weights.mat')\n",
    "Theta1 = weights['Theta1']\n",
    "Theta2 = weights['Theta2']\n",
    "Theta1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 26)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10285,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unroll parameters \n",
    "nn_params_saved = np.concatenate((Theta1.flatten(), Theta2.flatten()))\n",
    "nn_params_saved.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feedforward and cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "### 4.1 Feedforward and cost function\n",
    "\n",
    "Now you will implement the cost function and gradient for the neural network. First, complete the code for the function `nnCostFunction` in the next cell to return the cost.\n",
    "\n",
    "Recall that the cost function for the neural network (without regularization) is:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right]$$\n",
    "\n",
    "where $h_\\theta \\left( x^{(i)} \\right)$ is computed as shown in the neural network figure above, and K = 10 is the total number of possible labels. Note that $h_\\theta(x^{(i)})_k = a_k^{(3)}$ is the activation (output\n",
    "value) of the $k^{th}$ output unit. Also, recall that whereas the original labels (in the variable y) were 0, 1, ..., 9, for the purpose of training a neural network, we need to encode the labels as vectors containing only values 0 or 1, so that\n",
    "\n",
    "$$ y = \n",
    "\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\\\vdots \\\\ 0 \\end{bmatrix}, \\quad\n",
    "\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad \\cdots  \\quad \\text{or} \\qquad\n",
    "\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "For example, if $x^{(i)}$ is an image of the digit 5, then the corresponding $y^{(i)}$ (that you should use with the cost function) should be a 10-dimensional vector with $y_5 = 1$, and the other elements equal to 0.\n",
    "\n",
    "You should implement the feedforward computation that computes $h_\\theta(x^{(i)})$ for every example $i$ and sum the cost over all examples. **Your code should also work for a dataset of any size, with any number of labels** (you can assume that there are always at least $K \\ge 3$ labels).\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "**Implementation Note:** The matrix $X$ contains the examples in rows (i.e., X[i,:] is the i-th training example $x^{(i)}$, expressed as a $n \\times 1$ vector.) When you complete the code in `nnCostFunction`, you will need to add the column of 1â€™s to the X matrix. The parameters for each unit in the neural network is represented in Theta1 and Theta2 as one row. Specifically, the first row of Theta1 corresponds to the first hidden unit in the second layer. You can use a for-loop over the examples to compute the cost.\n",
    "</div>\n",
    "<a id=\"nnCostFunction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    h = 1 / (1 + np.exp(-x))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a one-hot encoding, Y, of the target vector, y.\n",
    "Y = np.zeros((M, K), dtype='uint8') \n",
    "\n",
    "for i, row in enumerate(Y):\n",
    "    Y[i, y[i] - 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nn_cost_function(nn_params, X, Y, M, N, L, K):\n",
    "    \"\"\"\n",
    "    Implements the neural network cost function and gradient for a two layer neural \n",
    "    network which performs classification. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_params : array_like\n",
    "        The parameters for the neural network which are \"unrolled\" into \n",
    "        a vector. This needs to be converted back into the weight matrices Theta1\n",
    "        and Theta2.\n",
    "    \n",
    "    input_layer_size : int\n",
    "        Number of features for the input layer. 400\n",
    "    \n",
    "    hidden_layer_size : int\n",
    "        Number of hidden units in the second layer. 25\n",
    "    \n",
    "    num_labels : int\n",
    "        Total number of labels, or equivalently number of units in output layer. 10\n",
    "    \n",
    "    X : array_like\n",
    "        Input dataset. A matrix of shape (m x input_layer_size). 5000 * 400\n",
    "    \n",
    "    y : array_like\n",
    "        Dataset labels. A vector of shape (m,). 5000 * 1\n",
    "    \n",
    "    lambda_ : float, optional\n",
    "        Regularization parameter.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function at the current weight values.\n",
    "    \n",
    "    grad : array_like\n",
    "        An \"unrolled\" vector of the partial derivatives of the concatenatation of\n",
    "        neural network weights Theta1 and Theta2.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    You should complete the code by working through the following parts.\n",
    "    \n",
    "    - Part 1: Feedforward the neural network and return the cost in the \n",
    "              variable J. After implementing Part 1, you can verify that your\n",
    "              cost function computation is correct by verifying the cost\n",
    "              computed in the following cell.\n",
    "    \n",
    "    - Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "              Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "              the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "              Theta2_grad, respectively. After implementing Part 2, you can check\n",
    "              that your implementation is correct by running checkNNGradients provided\n",
    "              in the utils.py module.\n",
    "    \n",
    "              Note: The vector y passed into the function is a vector of labels\n",
    "                    containing values from 0..K-1. You need to map this vector into a \n",
    "                    binary vector of 1's and 0's to be used with the neural network\n",
    "                    cost function.\n",
    "     \n",
    "              Hint: We recommend implementing backpropagation using a for-loop\n",
    "                    over the training examples if you are implementing it for the \n",
    "                    first time.\n",
    "    \n",
    "    - Part 3: Implement regularization with the cost function and gradients.\n",
    "    \n",
    "              Hint: You can implement this around the code for\n",
    "                    backpropagation. That is, you can compute the gradients for\n",
    "                    the regularization separately and then add them to Theta1_grad\n",
    "                    and Theta2_grad from Part 2.\n",
    "    \n",
    "    Note \n",
    "    ----\n",
    "    We have provided an implementation for the sigmoid function in the file \n",
    "    `utils.py` accompanying this assignment.\n",
    "    \"\"\"\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    # Unroll the parameter vector.\n",
    "    \n",
    "\n",
    "    Theta_1 = nn_params_saved[:(L - 1) * (N + 1)].reshape(L - 1, N + 1)\n",
    "    Theta_2 = nn_params_saved[(L - 1) * ( N + 1):].reshape(K, L)\n",
    "\n",
    "\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    \n",
    "    # Calculate activations in the second layer.\n",
    "    a_2 = sigmoid(Theta_1.dot(X.T))\n",
    "    \n",
    "    # Add the second layer's bias node.\n",
    "    a_2_p = np.vstack((np.ones(M), a_2))\n",
    "    \n",
    "    # Calculate the activation of the third layer.\n",
    "    a_3 = sigmoid(Theta_2.dot(a_2_p))\n",
    "    \n",
    "    # Calculate the cost function.\n",
    "    cost = 1 / M * np.trace(- Y.dot(np.log(a_3)) - (1 - Y).dot(np.log(1 - a_3)))\n",
    "    \n",
    "    \n",
    "    # ================================================================\n",
    "\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Regularized cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "### 1.4 Regularized cost function\n",
    "\n",
    "The cost function for neural networks with regularization is given by:\n",
    "\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right] + \\frac{\\lambda}{2 m} \\left[ \\sum_{j=1}^{25} \\sum_{k=1}^{400} \\left( \\Theta_{j,k}^{(1)} \\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} \\left( \\Theta_{j,k}^{(2)} \\right)^2 \\right] $$\n",
    "\n",
    "You can assume that the neural network will only have 3 layers - an input layer, a hidden layer and an output layer. However, your code should work for any number of input units, hidden units and outputs units. While we\n",
    "have explicitly listed the indices above for $\\Theta^{(1)}$ and $\\Theta^{(2)}$ for clarity, do note that your code should in general work with $\\Theta^{(1)}$ and $\\Theta^{(2)}$ of any size. Note that you should not be regularizing the terms that correspond to the bias. For the matrices `Theta1` and `Theta2`, this corresponds to the first column of each matrix. You should now add regularization to your cost function. Notice that you can first compute the unregularized cost function $J$ using your existing `nnCostFunction` and then later add the cost for the regularization terms.\n",
    "\n",
    "[Click here to go back to `nnCostFunction` for editing.](#nnCostFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cost_function_reg(nn_params, X, Y, M, N, L, K, lam):\n",
    "    \"\"\"Python version of nnCostFunction.m after completing 'Part 1' with regularization added.\"\"\"\n",
    "\n",
    "    # Unroll the parameter vector.\n",
    "    theta_1 = nn_params[:(L - 1) * (N + 1)].reshape(L - 1, N + 1)\n",
    "    theta_2 = nn_params[(L - 1) * (N + 1):].reshape(K, L)\n",
    "    \n",
    "    # Calculate activations in the second layer.\n",
    "    a_2 = sigmoid(theta_1.dot(X.T))\n",
    "    \n",
    "    # Add the second layer's bias node.\n",
    "    a_2_p = np.vstack((np.ones(M), a_2))\n",
    "    \n",
    "    # Calculate the activation of the third layer.\n",
    "    a_3 = sigmoid(theta_2.dot(a_2_p))\n",
    "    \n",
    "    # Calculate the cost function with the addition of a regularization term.\n",
    "    cost = 1 / M * np.trace(- Y.dot(np.log(a_3)) - (1 - Y).dot(np.log(1 - a_3))) \\\n",
    "        + lam / 2 / M * (np.sum(theta_1[:, 1:] * theta_1[:, 1:]) + np.sum(theta_2[:, 1:] * theta_2[:, 1:]))\n",
    "            \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost at parameters (loaded from ex4weights): 0.000000\n",
      "(this value should be about 0.383770)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost_saved_reg = nn_cost_function_reg(nn_params_saved, X, Y, M, N, L, K, 1)\n",
    "\n",
    "print 'Regularized cost at parameters (loaded from ex4weights): %.6f' % cost_saved_reg\n",
    "print '(this value should be about 0.383770)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Sigmoid Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To help you get started with this part of the exercise, you will first implement\n",
    "the sigmoid gradient function. The gradient for the sigmoid function can be\n",
    "computed as\n",
    "\n",
    "$$ g'(z) = \\frac{d}{dz} g(z) = g(z)\\left(1-g(z)\\right) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\text{sigmoid}(z) = g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "Now complete the implementation of `sigmoidGradient` in the next cell.\n",
    "<a id=\"sigmoidGradient\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the sigmoid function evaluated at z. \n",
    "    This should work regardless if z is a matrix or a vector. \n",
    "    In particular, if z is a vector or matrix, you should return\n",
    "    the gradient for each element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        A vector or matrix as input to the sigmoid function. \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    g : array_like\n",
    "        Gradient of the sigmoid function. Has the same shape as z. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the gradient of the sigmoid function evaluated at\n",
    "    each value of z (z can be a matrix, vector or scalar).\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    We have provided an implementation of the sigmoid function \n",
    "    in `utils.py` file accompanying this assignment.\n",
    "    \"\"\"\n",
    "\n",
    "    g = sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]: 0.197, 0.235, 0.250, 0.235, 0.197\n",
      "(should be an even function)\n"
     ]
    }
   ],
   "source": [
    "print 'Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:', \\\n",
    "    ', '.join('%.3f' % item for item in sigmoidGradient(np.array([-1, -0.5, 0, 0.5, 1])))\n",
    "print '(should be an even function)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Random Initialization\n",
    "\n",
    "When training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $\\Theta^{(l)}$ uniformly in the range $[-\\epsilon_{init}, \\epsilon_{init}]$. You should use $\\epsilon_{init} = 0.12$. This range of values ensures that the parameters are kept small and makes the learning more efficient.\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "One effective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network. A good choice of $\\epsilon_{init}$ is $\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}}$ where $L_{in} = s_l$ and $L_{out} = s_{l+1}$ are the number of units in the layers adjacent to $\\Theta^{l}$.\n",
    "</div>\n",
    "\n",
    "Your job is to complete the function `randInitializeWeights` to initialize the weights for $\\Theta$. Modify the function by filling in the following code:\n",
    "\n",
    "```python\n",
    "# Randomly initialize the weights to small values\n",
    "W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "```\n",
    "Note that we give the function an argument for $\\epsilon$ with default value `epsilon_init = 0.12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_init = 12\n",
    "theta_1_0 = np.random.uniform(-eps_init, eps_init, Theta_1.shape)\n",
    "theta_2_0 = np.random.uniform(-eps_init, eps_init, Theta_2.shape)\n",
    "# unroll the parameters\n",
    "nn_params_0 = np.concatenate((theta_1_0.flatten(), theta_2_0.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "### 2.4 Backpropagation\n",
    "\n",
    "![](Figures/ex4-backpropagation.png)\n",
    "\n",
    "Now, you will implement the backpropagation algorithm. Recall that the intuition behind the backpropagation algorithm is as follows. Given a training example $(x^{(t)}, y^{(t)})$, we will first run a â€œforward passâ€ to compute all the activations throughout the network, including the output value of the hypothesis $h_\\theta(x)$. Then, for each node $j$ in layer $l$, we would like to compute an â€œerror termâ€ $\\delta_j^{(l)}$ that measures how much that node was â€œresponsibleâ€ for any errors in our output.\n",
    "\n",
    "For an output node, we can directly measure the difference between the networkâ€™s activation and the true target value, and use that to define $\\delta_j^{(3)}$ (since layer 3 is the output layer). For the hidden units, you will compute $\\delta_j^{(l)}$ based on a weighted average of the error terms of the nodes in layer $(l+1)$. In detail, here is the backpropagation algorithm (also depicted in the figure above). You should implement steps 1 to 4 in a loop that processes one example at a time. Concretely, you should implement a for-loop `for t in range(m)` and place steps 1-4 below inside the for-loop, with the $t^{th}$ iteration performing the calculation on the $t^{th}$ training example $(x^{(t)}, y^{(t)})$. Step 5 will divide the accumulated gradients by $m$ to obtain the gradients for the neural network cost function.\n",
    "\n",
    "1. Set the input layerâ€™s values $(a^{(1)})$ to the $t^{th }$training example $x^{(t)}$. Perform a feedforward pass, computing the activations $(z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)})$ for layers 2 and 3. Note that you need to add a `+1` term to ensure that the vectors of activations for layers $a^{(1)}$ and $a^{(2)}$ also include the bias unit. In `numpy`, if a 1 is a column matrix, adding one corresponds to `a_1 = np.concatenate([np.ones((m, 1)), a_1], axis=1)`.\n",
    "\n",
    "1. For each output unit $k$ in layer 3 (the output layer), set \n",
    "$$\\delta_k^{(3)} = \\left(a_k^{(3)} - y_k \\right)$$\n",
    "where $y_k \\in \\{0, 1\\}$ indicates whether the current training example belongs to class $k$ $(y_k = 1)$, or if it belongs to a different class $(y_k = 0)$. You may find logical arrays helpful for this task (explained in the previous programming exercise).\n",
    "\n",
    "1. For the hidden layer $l = 2$, set \n",
    "$$ \\delta^{(2)} = \\left( \\Theta^{(2)} \\right)^T \\delta^{(3)} * g'\\left(z^{(2)} \\right)$$\n",
    "Note that the symbol $*$ performs element wise multiplication in `numpy`.\n",
    "\n",
    "1. Accumulate the gradient from this example using the following formula. Note that you should skip or remove $\\delta_0^{(2)}$. In `numpy`, removing $\\delta_0^{(2)}$ corresponds to `delta_2 = delta_2[1:]`.\n",
    "\n",
    "1. Obtain the (unregularized) gradient for the neural network cost function by dividing the accumulated gradients by $\\frac{1}{m}$:\n",
    "$$ \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)}$$\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "**Python/Numpy tip**: You should implement the backpropagation algorithm only after you have successfully completed the feedforward and cost functions. While implementing the backpropagation alogrithm, it is often useful to use the `shape` function to print out the shapes of the variables you are working with if you run into dimension mismatch errors.\n",
    "</div>\n",
    "\n",
    "[Click here to go back and update the function `nnCostFunction` with the backpropagation algorithm](#nnCostFunction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cost_function_grad(nn_params, X, Y, M, N, L, K, lam):\n",
    "    \"\"\"Python version of nnCostFunction.m after completing 'Part 2' \n",
    "    (the cost function is regularized here but not the gradient).\"\"\"    \n",
    "    \n",
    "    # Unroll the parameter vector.\n",
    "    theta_1 = nn_params[:(L - 1) * (N + 1)].reshape(L - 1, N + 1)\n",
    "    theta_2 = nn_params[(L - 1) * (N + 1):].reshape(K, L)\n",
    "    \n",
    "    \"\"\"Feedforward pass.\"\"\"\n",
    "    # Calculate activations in the second layer (as well as z_2, which is needed below).\n",
    "    z_2 = theta_1.dot(X.T)\n",
    "    a_2_p = np.vstack((np.ones(M), sigmoid(z_2)))\n",
    "    \n",
    "    # Calculate the activation of the third layer.\n",
    "    a_3 = sigmoid(theta_2.dot(a_2_p))\n",
    "    \n",
    "    # Calculate the cost function with the addition of a regularization term.\n",
    "    cost = 1 / M * np.trace(- Y.dot(np.log(a_3)) - (1 - Y).dot(np.log(1 - a_3))) \\\n",
    "        + lam / 2 / M * (np.sum(theta_1[:, 1:] * theta_1[:, 1:]) + np.sum(theta_2[:, 1:] * theta_2[:, 1:])) \n",
    "    \n",
    "    \"\"\"Backpropagation (use the chain rule).\"\"\"\n",
    "    # Calculate the gradient for parameters in the third layer.\n",
    "    grad_theta_2 = 1 / M * (a_3 - Y.T).dot(a_2_p.T) \n",
    "    \n",
    "        # Calculate the gradient for parameters in the second layer.\n",
    "    theta_delta = theta_2_0[:, 1:].T.dot(a_3 - Y.T)\n",
    "    s_g_z_2 = sigmoidGradient(z_2)\n",
    "    grad_theta_1 = 1 / M * np.array([[np.sum(theta_delta[p] * s_g_z_2[p] * X.T[q]) \n",
    "        for q in xrange(N+1)] for p in xrange(L-1)]) \n",
    "    return cost, np.concatenate((grad_theta_1.flatten(), grad_theta_2.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you have implemented the backpropagation algorithm, we will proceed to run gradient checking on your implementation. \n",
    "#The gradient check will allow you to increase your confidence that your code is computing the gradients correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have implemented the backpropagation algorithm, we will proceed to run gradient checking on your implementation. The gradient check will allow you to increase your confidence that your code is\n",
    "computing the gradients correctly.\n",
    "\n",
    "### 2.4  Gradient checking \n",
    "\n",
    "In your neural network, you are minimizing the cost function $J(\\Theta)$. To perform gradient checking on your parameters, you can imagine â€œunrollingâ€ the parameters $\\Theta^{(1)}$, $\\Theta^{(2)}$ into a long vector $\\theta$. By doing so, you can think of the cost function being $J(\\Theta)$ instead and use the following gradient checking procedure.\n",
    "\n",
    "Suppose you have a function $f_i(\\theta)$ that purportedly computes $\\frac{\\partial}{\\partial \\theta_i} J(\\theta)$; youâ€™d like to check if $f_i$ is outputting correct derivative values.\n",
    "\n",
    "$$\n",
    "\\text{Let } \\theta^{(i+)} = \\theta + \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ \\epsilon \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad \\theta^{(i-)} = \\theta - \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ \\epsilon \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, $\\theta^{(i+)}$ is the same as $\\theta$, except its $i^{th}$ element has been incremented by $\\epsilon$. Similarly, $\\theta^{(iâˆ’)}$ is the corresponding vector with the $i^{th}$ element decreased by $\\epsilon$. You can now numerically verify $f_i(\\theta)$â€™s correctness by checking, for each $i$, that:\n",
    "\n",
    "$$ f_i\\left( \\theta \\right) \\approx \\frac{J\\left( \\theta^{(i+)}\\right) - J\\left( \\theta^{(i-)} \\right)}{2\\epsilon} $$\n",
    "\n",
    "The degree to which these two values should approximate each other will depend on the details of $J$. But assuming $\\epsilon = 10^{-4}$, youâ€™ll usually find that the left- and right-hand sides of the above will agree to at least 4 significant digits (and often many more).\n",
    "\n",
    "We have implemented the function to compute the numerical gradient for you in `computeNumericalGradient` (within the file `utils.py`). While you are not required to modify the file, we highly encourage you to take a look at the code to understand how it works.\n",
    "\n",
    "In the next cell we will run the provided function `checkNNGradients` which will create a small neural network and dataset that will be used for checking your gradients. If your backpropagation implementation is correct,\n",
    "you should see a relative difference that is less than 1e-9.\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "**Practical Tip**: When performing gradient checking, it is much more efficient to use a small neural network with a relatively small number of input units and hidden units, thus having a relatively small number\n",
    "of parameters. Each dimension of $\\theta$ requires two evaluations of the cost function and this can be expensive. In the function `checkNNGradients`, our code creates a small random model and dataset which is used with `computeNumericalGradient` for gradient checking. Furthermore, after you are confident that your gradient computations are correct, you should turn off gradient checking before running your learning algorithm.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "**Practical Tip:** Gradient checking works for any function where you are computing the cost and the gradient. Concretely, you can use the same `computeNumericalGradient` function to check if your gradient implementations for the other exercises are correct too (e.g., logistic regressionâ€™s cost function).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
